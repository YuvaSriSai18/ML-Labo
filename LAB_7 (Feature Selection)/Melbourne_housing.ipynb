{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GgNaPXHzMg16",
        "outputId": "c861c27e-ada5-4227-ab63-a9569c834867"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Shape of the Data: (34857, 21)\n",
            "Missing Values in Each Column:\n",
            " Suburb               0\n",
            "Address              0\n",
            "Rooms                0\n",
            "Type                 0\n",
            "Price             7610\n",
            "Method               0\n",
            "SellerG              0\n",
            "Date                 0\n",
            "Distance             1\n",
            "Postcode             1\n",
            "Bedroom2          8217\n",
            "Bathroom          8226\n",
            "Car               8728\n",
            "Landsize         11810\n",
            "BuildingArea     21115\n",
            "YearBuilt        19306\n",
            "CouncilArea          3\n",
            "Lattitude         7976\n",
            "Longtitude        7976\n",
            "Regionname           3\n",
            "Propertycount        3\n",
            "dtype: int64\n",
            "\n",
            "Shape after filtering out columns with >20% missing values: (34857, 12)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_csv(r\"Melbourne_housing.csv\")\n",
        "\n",
        "\n",
        "# Display basic info\n",
        "print(\"Initial Shape of the Data:\", df.shape)\n",
        "print(\"Missing Values in Each Column:\\n\", df.isnull().sum())\n",
        "\n",
        "# Filter out columns where more than 20% of values are missing\n",
        "missing_threshold = 20\n",
        "missing_ratio = df.isnull().mean() * 100\n",
        "filtered_data = df.loc[:, missing_ratio <= missing_threshold]\n",
        "\n",
        "# Display the shape after filtering\n",
        "print(\"\\nShape after filtering out columns with >20% missing values:\", filtered_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "High Correlation"
      ],
      "metadata": {
        "id": "YJ4AqDuHOT9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"Melbourne_housing.csv\")\n",
        "\n",
        "# Handle missing values by dropping rows with missing values\n",
        "df_cleaned = df.dropna()\n",
        "\n",
        "# Remove non-numeric columns before calculating the correlation matrix\n",
        "df_numeric = df_cleaned.select_dtypes(include=[np.number])\n",
        "\n",
        "# Compute the correlation matrix for numeric columns only\n",
        "correlation_matrix = df_numeric.corr()\n",
        "\n",
        "# Identify and remove highly correlated features (correlation > 0.85)\n",
        "threshold = 0.85\n",
        "high_corr_features = np.where(correlation_matrix > threshold)\n",
        "\n",
        "# Find pairs of highly correlated features\n",
        "high_corr_pairs = [(correlation_matrix.index[x], correlation_matrix.columns[y])\n",
        "                   for x, y in zip(*high_corr_features) if x != y and x < y]\n",
        "\n",
        "print(\"Highly correlated features (correlation > 0.85):\\n\", high_corr_pairs)\n",
        "\n",
        "# Remove one feature from each highly correlated pair\n",
        "features_to_remove = set([pair[1] for pair in high_corr_pairs])\n",
        "df_reduced = df_cleaned.drop(columns=features_to_remove)\n",
        "\n",
        "# Convert categorical variables into numerical values using OneHotEncoder\n",
        "df_reduced = pd.get_dummies(df_reduced, drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df_reduced.drop(columns=['Price'])\n",
        "y = df_reduced['Price']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"R-squared: {r2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iFx9-Py4OZaA",
        "outputId": "2e2d839b-3c0a-4248-dfbb-dda2f6853e19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Highly correlated features (correlation > 0.85):\n",
            " [('Rooms', 'Bedroom2')]\n",
            "Mean Squared Error: 120300724794.60493\n",
            "R-squared: 0.6868892150793516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Low Variance Filtered\n"
      ],
      "metadata": {
        "id": "UCoqKnO6Tzu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handle missing values by dropping rows with missing values\n",
        "df_cleaned = df.dropna()\n",
        "# Convert categorical variables into numerical values using OneHotEncoder or get_dummies\n",
        "df_encoded = pd.get_dummies(df_cleaned, drop_first=True)\n",
        "X = df_encoded.drop(columns=['Price'])\n",
        "y = df_encoded['Price']\n",
        "# Calculate variance of all features\n",
        "feature_variances = X.var()\n",
        "# print(\"Variance of features before filtering:\")\n",
        "# print(feature_variances)\n",
        "\n",
        "# Apply Variance Threshold (Filter out features with variance below the threshold, e.g., 0.01)\n",
        "variance_threshold = VarianceThreshold(threshold=0.01)\n",
        "X_high_variance = variance_threshold.fit_transform(X)\n",
        "# Get the indices of the features that were kept and removed\n",
        "features_kept = X.columns[variance_threshold.get_support()]\n",
        "features_removed = X.columns[~variance_threshold.get_support()]\n",
        "print(\"\\nFeatures removed after applying variance threshold:\")\n",
        "print(features_removed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T13j8KpOTy6I",
        "outputId": "a2570bbe-825c-4632-ac75-2b220b97e9c4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Features removed after applying variance threshold:\n",
            "Index(['Lattitude', 'Suburb_Aberfeldie', 'Suburb_Airport West',\n",
            "       'Suburb_Albanvale', 'Suburb_Albert Park', 'Suburb_Albion',\n",
            "       'Suburb_Alphington', 'Suburb_Altona', 'Suburb_Altona Meadows',\n",
            "       'Suburb_Altona North',\n",
            "       ...\n",
            "       'CouncilArea_Frankston City Council',\n",
            "       'CouncilArea_Greater Dandenong City Council',\n",
            "       'CouncilArea_Macedon Ranges Shire Council',\n",
            "       'CouncilArea_Mitchell Shire Council',\n",
            "       'CouncilArea_Moorabool Shire Council',\n",
            "       'CouncilArea_Nillumbik Shire Council',\n",
            "       'CouncilArea_Yarra Ranges Shire Council', 'Regionname_Eastern Victoria',\n",
            "       'Regionname_Northern Victoria', 'Regionname_Western Victoria'],\n",
            "      dtype='object', length=9315)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "\n",
        "# Initialize Sequential Feature Selector for forward selection\n",
        "# Reduce number of folds for cross-validation (from 5 to 3) and limit features to select\n",
        "sfs = SequentialFeatureSelector(model, n_features_to_select=10, direction=\"forward\", cv=3)\n",
        "\n",
        "# Fit the feature selector to the training data\n",
        "sfs.fit(X_train, y_train)\n",
        "\n",
        "# Get the selected features\n",
        "selected_features = sfs.get_support(indices=True)\n",
        "\n",
        "# Retrieve the feature names for the selected features\n",
        "selected_feature_names = X.columns[selected_features]\n",
        "print(\"Selected feature names:\", selected_feature_names)\n",
        "\n",
        "# Use only selected features for training and testing\n",
        "X_train_selected = X_train.iloc[:, selected_features]\n",
        "X_test_selected = X_test.iloc[:, selected_features]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaHIKiZNg3uz",
        "outputId": "17c802f0-68fc-41a8-ede1-6ca9847ae994"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected feature names: Index(['Rooms', 'Distance', 'Bedroom2', 'Landsize', 'BuildingArea',\n",
            "       'Lattitude', 'Suburb_Briar Hill', 'Type_t', 'SellerG_Mandy',\n",
            "       'Regionname_South-Eastern Metropolitan'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"Melbourne_housing.csv\")\n",
        "\n",
        "# Handle missing values by dropping rows with missing values\n",
        "df_cleaned = df.dropna()\n",
        "\n",
        "# Convert categorical variables into numerical values using OneHotEncoder\n",
        "df_encoded = pd.get_dummies(df_cleaned, drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df_encoded.drop(columns=['Price'])\n",
        "y = df_encoded['Price']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Function to perform backward feature elimination\n",
        "def backward_feature_elimination(X_train, X_test, y_train, y_test, model):\n",
        "    features = X_train.columns.tolist()\n",
        "    scores = []\n",
        "\n",
        "    while len(features) > 0:\n",
        "        # Train the model with current features\n",
        "        model.fit(X_train[features], y_train)\n",
        "        # Make predictions\n",
        "        y_pred = model.predict(X_test[features])\n",
        "        # Calculate and store the score (R-squared)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        scores.append((len(features), r2))\n",
        "\n",
        "        # Get feature importances\n",
        "        importances = model.feature_importances_\n",
        "        # Create a DataFrame for feature importance\n",
        "        importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
        "        # Remove the least important feature\n",
        "        least_important_feature = importance_df.nsmallest(1, 'Importance')['Feature'].values[0]\n",
        "        features.remove(least_important_feature)\n",
        "\n",
        "        print(f\"Removed feature: {least_important_feature} | Remaining features: {len(features)}\")\n",
        "\n",
        "    return scores\n",
        "\n",
        "# Run backward feature elimination\n",
        "scores = backward_feature_elimination(X_train, X_test, y_train, y_test, model)\n",
        "\n",
        "# Plot the results\n",
        "num_features, r2_scores = zip(*scores)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(num_features, r2_scores, marker='o')\n",
        "plt.title('Backward Feature Elimination: R-squared vs Number of Features')\n",
        "plt.xlabel('Number of Features')\n",
        "plt.ylabel('R-squared Score')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DF7bC3vj_PBu",
        "outputId": "aba90c74-98b3-4de7-e718-015ceb075d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed feature: Suburb_Bacchus Marsh | Remaining features: 9458\n",
            "Removed feature: Suburb_Beaconsfield | Remaining features: 9457\n",
            "Removed feature: Suburb_Beaconsfield Upper | Remaining features: 9456\n",
            "Removed feature: Suburb_Kooyong | Remaining features: 9455\n",
            "Removed feature: Suburb_Plumpton | Remaining features: 9454\n",
            "Removed feature: Suburb_Sandhurst | Remaining features: 9453\n",
            "Removed feature: Suburb_Waterways | Remaining features: 9452\n",
            "Removed feature: Suburb_Chirnside Park | Remaining features: 9451\n",
            "Removed feature: Suburb_Healesville | Remaining features: 9450\n",
            "Removed feature: Suburb_Wattle Glen | Remaining features: 9449\n",
            "Removed feature: Address_1 Alma St | Remaining features: 9448\n",
            "Removed feature: Address_1 Caroline St | Remaining features: 9447\n",
            "Removed feature: Address_1 Chisholm St | Remaining features: 9446\n",
            "Removed feature: Address_1 Christie St | Remaining features: 9445\n",
            "Removed feature: Suburb_Hurstbridge | Remaining features: 9444\n",
            "Removed feature: Address_1 Auckland St | Remaining features: 9443\n",
            "Removed feature: Suburb_Gisborne South | Remaining features: 9442\n",
            "Removed feature: Address_1 Balloan St | Remaining features: 9441\n",
            "Removed feature: Suburb_Seabrook | Remaining features: 9440\n",
            "Removed feature: Address_1 College St | Remaining features: 9439\n",
            "Removed feature: Address_1 Delaware St | Remaining features: 9438\n",
            "Removed feature: Address_1 Denison Ct | Remaining features: 9437\n",
            "Removed feature: Address_1 Dongola Rd | Remaining features: 9436\n",
            "Removed feature: Address_1 Dunlavin Rd | Remaining features: 9435\n",
            "Removed feature: Address_1 Fernhill Ct | Remaining features: 9434\n",
            "Removed feature: Address_1 Bloomfield Rd | Remaining features: 9433\n",
            "Removed feature: Address_1 Florence St | Remaining features: 9432\n",
            "Removed feature: Address_1 Haddington Cr | Remaining features: 9431\n",
            "Removed feature: Address_1 Harding St | Remaining features: 9430\n",
            "Removed feature: Address_1 Chester Cl | Remaining features: 9429\n",
            "Removed feature: Address_1 Howell St | Remaining features: 9428\n",
            "Removed feature: Suburb_Keilor Lodge | Remaining features: 9427\n",
            "Removed feature: Address_1 Howson Ct | Remaining features: 9426\n",
            "Removed feature: Address_1 Meadowbank St | Remaining features: 9425\n",
            "Removed feature: Suburb_Officer | Remaining features: 9424\n",
            "Removed feature: Address_1 Montana St | Remaining features: 9423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(\"Melbourne_housing.csv\")\n",
        "\n",
        "# Handle missing values by dropping rows with missing values\n",
        "df_cleaned = df.dropna()\n",
        "\n",
        "# Convert categorical variables into numerical values using OneHotEncoder\n",
        "df_encoded = pd.get_dummies(df_cleaned, drop_first=True)\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = df_encoded.drop(columns=['Price'])\n",
        "y = df_encoded['Price']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate performance metrics\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "print(f\"R-squared: {r2}\")\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Create a DataFrame to hold feature names and their importances\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Display feature importances\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(feature_importance_df)\n",
        "\n",
        "# Plot feature importances\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.barh(feature_importance_df['Feature'][:10], feature_importance_df['Importance'][:10], color='blue')\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 10 Feature Importances')\n",
        "plt.gca().invert_yaxis()  # Invert y-axis for better visualization\n",
        "plt.show()\n",
        "\n",
        "# Function to remove least important features and assess model accuracy\n",
        "def remove_least_important_features(X_train, X_test, y_train, y_test, model, feature_importance_df, threshold=0.01):\n",
        "    # Keep features with importance above the threshold\n",
        "    important_features = feature_importance_df[feature_importance_df['Importance'] > threshold]['Feature'].tolist()\n",
        "\n",
        "    # Train and evaluate the model on selected features\n",
        "    model.fit(X_train[important_features], y_train)\n",
        "    y_pred_reduced = model.predict(X_test[important_features])\n",
        "\n",
        "    mse_reduced = mean_squared_error(y_test, y_pred_reduced)\n",
        "    r2_reduced = r2_score(y_test, y_pred_reduced)\n",
        "\n",
        "    print(f\"\\nMean Squared Error after removing least important features: {mse_reduced}\")\n",
        "    print(f\"R-squared after removing least important features: {r2_reduced}\")\n",
        "    return important_features\n",
        "\n",
        "# Call the function to assess model accuracy after removing least important features\n",
        "remaining_features = remove_least_important_features(X_train, X_test, y_train, y_test, model, feature_importance_df)\n",
        "print(\"\\nRemaining features after removal:\", remaining_features)\n"
      ],
      "metadata": {
        "id": "cdN3dtpVWWt5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}